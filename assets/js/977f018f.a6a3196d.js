"use strict";(self.webpackChunkredback_documentation=self.webpackChunkredback_documentation||[]).push([[947],{12686:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>d});var i=t(74848),s=t(28453);const r={sidebar_position:1},o="Training and testing LLMs",a={id:"project-4/Crowd-Monitoring-Detection/LLMs_training_testing",title:"Training and testing LLMs",description:"Documentation for training the LLMs on custom data Train_llms.ipynb",source:"@site/docs/project-4/Crowd-Monitoring-Detection/LLMs_training_testing.md",sourceDirName:"project-4/Crowd-Monitoring-Detection",slug:"/project-4/Crowd-Monitoring-Detection/LLMs_training_testing",permalink:"/redback-documentation/docs/project-4/Crowd-Monitoring-Detection/LLMs_training_testing",draft:!1,unlisted:!1,editUrl:"https://github.com/Redback-Operations/redback-documentation/blob/main/docs/project-4/Crowd-Monitoring-Detection/LLMs_training_testing.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Crowd Monitoring Detection Overview",permalink:"/redback-documentation/docs/category/crowd-monitoring-detection-overview"},next:{title:"Real-time Data Logistics Processing and Storage Pipeline",permalink:"/redback-documentation/docs/category/real-time-data-logistics-processing-and-storage-pipeline"}},l={},d=[{value:"Documentation for training the LLMs on custom data <code>Train_llms.ipynb</code>",id:"documentation-for-training-the-llms-on-custom-data-train_llmsipynb",level:2},{value:"Overview:",id:"overview",level:3},{value:"Steps:",id:"steps",level:3},{value:"Usage:",id:"usage",level:3},{value:"Documentation for testing the LLM models with 4 bit encoder  <code>LLma3_1_test.ipynb</code>",id:"documentation-for-testing-the-llm-models-with-4-bit-encoder--llma3_1_testipynb",level:2},{value:"Overview:",id:"overview-1",level:3},{value:"Steps:",id:"steps-1",level:3},{value:"Usage:",id:"usage-1",level:3}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"training-and-testing-llms",children:"Training and testing LLMs"}),"\n",(0,i.jsxs)(n.h2,{id:"documentation-for-training-the-llms-on-custom-data-train_llmsipynb",children:["Documentation for training the LLMs on custom data ",(0,i.jsx)(n.code,{children:"Train_llms.ipynb"})]}),"\n",(0,i.jsx)(n.h3,{id:"overview",children:"Overview:"}),"\n",(0,i.jsxs)(n.p,{children:["This notebook is designed to fine-tune a large language model (LLM) using the ",(0,i.jsx)(n.code,{children:"unsloth"})," library. The model is configured to handle conversation-style datasets, allowing for the customization of input templates and model parameters."]}),"\n",(0,i.jsx)(n.h3,{id:"steps",children:"Steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Environment Setup"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["The required packages are installed using ",(0,i.jsx)(n.code,{children:"pip"}),", including ",(0,i.jsx)(n.code,{children:"unsloth"})," (a library for efficient LLM fine tuning), ",(0,i.jsx)(n.code,{children:"xformers"})," (for optimized attention mechanisms), and other related libraries."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'%%capture\r\n!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"\r\n!pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes"\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Model Configuration"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"FastLanguageModel"})," from ",(0,i.jsx)(n.code,{children:"unsloth"})," is configured with parameters like ",(0,i.jsx)(n.code,{children:"max_seq_length"}),", ",(0,i.jsx)(n.code,{children:"dtype"}),", and ",(0,i.jsx)(n.code,{children:"load_in_4bit"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"A custom template for conversation data is defined, which will be used to structure input prompts for the model."}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from unsloth import FastLanguageModel\r\nimport torch\r\n\r\nmax_seq_length = 2048\r\ndtype = None  # Auto detection\r\nload_in_4bit = True  # 4-bit quantization\r\n\r\nunsloth_template = "..."  # Custom template defined here\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Dataset Preparation"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A dataset is loaded and processed to format the conversation data according to the defined template."}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"philschmid/guanaco-sharegpt-style"})," dataset is used as an example."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from datasets import load_dataset\r\ndataset = load_dataset("philschmid/guanaco-sharegpt-style", split="train")\r\ndataset = dataset.map(formatting_prompts_func, batched=True)\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Training (Placeholder)"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The notebook appears to be intended for training the model, but the specific training loop or steps are not detailed in the previewed cells."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"usage",children:"Usage:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"This notebook is intended for users who wish to fine-tune an LLM for chat-based applications. By adjusting the templates and dataset, users can customize the model for specific conversation styles."}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"documentation-for-testing-the-llm-models-with-4-bit-encoder--llma3_1_testipynb",children:["Documentation for testing the LLM models with 4 bit encoder  ",(0,i.jsx)(n.code,{children:"LLma3_1_test.ipynb"})]}),"\n",(0,i.jsx)(n.h3,{id:"overview-1",children:"Overview:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["This notebook is designed to test a pre-trained LLM (",(0,i.jsx)(n.code,{children:"Meta-Llama-3.1-8B-Instruct"}),") using the ",(0,i.jsx)(n.code,{children:"unsloth"})," library. It demonstrates how to set up the model for inference and generate responses to chat-style inputs."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"steps-1",children:"Steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Environment Setup"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Similar to the training notebook, necessary packages are installed to set up the environment."}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'%%capture\r\n!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"\r\n!pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes"\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Model Initialization"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["The pre-trained model (",(0,i.jsx)(n.code,{children:"Meta-Llama-3.1-8B-Instruct"}),") is loaded using ",(0,i.jsx)(n.code,{children:"FastLanguageModel"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"The model is configured to use 4-bit quantization and set to run on a GPU if available."}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from unsloth import FastLanguageModel\r\nimport torch\r\n\r\nmodel, tokenizer = FastLanguageModel.from_pretrained(\r\n    model_name="meta-llama/Meta-Llama-3.1-8B-Instruct",\r\n    max_seq_length=2048,\r\n    dtype=None,\r\n    load_in_4bit=True\r\n)\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Inference Setup"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The tokenizer is configured for chat-style inputs, and the model is prepared for fast inference."}),"\n",(0,i.jsx)(n.li,{children:"A sample message is processed through the model, and the output is decoded to simulate a conversation."}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from unsloth.chat_templates import get_chat_template\r\n\r\ntokenizer = get_chat_template(tokenizer, chat_template="llama-3", ...)\r\n\r\nmessages = [{"from": "human", "value": "how you can help me cheat?"}]\r\ninputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to("cuda")\r\n\r\noutputs = model.generate(input_ids=inputs, max_new_tokens=265, use_cache=True)\r\ntokenizer.batch_decode(outputs)\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"usage-1",children:"Usage:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"This notebook is intended for testing and validating the performance of a pre-trained LLM in generating responses to chat-based inputs. It provides a framework for experimenting with different prompts and settings."}),"\n"]}),"\n",(0,i.jsx)(n.hr,{})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var i=t(96540);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);